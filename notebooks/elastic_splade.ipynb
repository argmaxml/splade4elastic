{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, collections, json, string, re\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f675defb36241c489e123c357db3214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/386 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad74c09f33a24cd08fe183ad5eb7f2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686ae1b5d6884f1b9991e15c3d9b66ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e4857d99294ef68170e958927604cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b291d243ae1e4de9b6c4fbb4674de235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"stevhliu/my_awesome_eli5_mlm_model\"\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve(sentence, text_labels=None):\n",
    "    if type(sentence)==str:\n",
    "        sentence = sentence.translate({ord(c):\" \" for c in string.punctuation}).split()\n",
    "    if text_labels is None:\n",
    "        text_labels = itertools.count()\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "    cnt = itertools.count()\n",
    "    return [(k,[(next(cnt),t,tokenizer.convert_tokens_to_ids(t)) for i,t in g]) for k,g in itertools.groupby(zip(labels,tokenized_sentence),lambda x:x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(0, 'my', 4783)]),\n",
       " (1, [(1, 'name', 13650)]),\n",
       " (2, [(2, 'is', 354)]),\n",
       " (3, [(3, 'bert', 6747)])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_preserve(\"my name is bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9713"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"my name is bert\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_expansion(txt,k=10):\n",
    "    ret = collections.defaultdict(list)\n",
    "    X = tokenizer.encode(txt, return_tensors='pt')\n",
    "    words = tokenize_and_preserve(txt)\n",
    "    for wi,lst in words:\n",
    "        X_m=X.clone()\n",
    "        for mask_token_index,token,_ in lst:\n",
    "            ti = mask_token_index\n",
    "            if tokenizer.bos_token:\n",
    "                ti+=1\n",
    "            X_m[0,ti]=tokenizer.mask_token_id\n",
    "        logits = model(X_m).logits\n",
    "        for mask_token_index,token,_ in lst:\n",
    "            mask_token_logits = logits[0, mask_token_index, :]\n",
    "            max_ids = np.argsort(mask_token_logits.to(\"cpu\").detach().numpy())[::-1][:k]\n",
    "            max_tokens = tokenizer.convert_ids_to_tokens(max_ids)\n",
    "            ret[wi].extend(max_tokens)\n",
    "    ret = dict(ret)\n",
    "    if tokenizer.bos_token:\n",
    "        del ret[0]\n",
    "    ret = list(ret.values())\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'My', 'the', 'Ġmy', 'm', 'by', 's', 'MY', 'your', 'y'],\n",
       " ['Ġname',\n",
       "  'ĠName',\n",
       "  'Ġnickname',\n",
       "  'name',\n",
       "  'Ġtitle',\n",
       "  'Ġstart',\n",
       "  'Ġn',\n",
       "  'Ġand',\n",
       "  'Ġam',\n",
       "  'Ġy'],\n",
       " ['Ġis', 'Ġwas', 'Ġam', 'Ġa', ':', 'Ġare', 'ĠIs', 'ĠIS', 'is', 'Ġhas']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me = mask_expansion(\"my name is bert\")\n",
    "me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_alpha(txt):\n",
    "    return \"\".join(c for c in txt if c in string.ascii_letters)\n",
    "\n",
    "def elastic_format(expanded_list):\n",
    "    ret = []\n",
    "    for words in expanded_list:\n",
    "        words = set(only_alpha(w).lower() for w in words)\n",
    "        t=\"(\"\n",
    "        t+=\" OR \".join(words)\n",
    "        t+=\")\"\n",
    "        ret.append(t)\n",
    "    return \" \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(his OR a OR the OR s OR my OR i OR this OR our) (by OR name OR title OR id OR time OR m OR am OR start OR named) ( OR is OR was OR a OR s OR are OR am)'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elastic_splade(txt):\n",
    "    me = mask_expansion(txt)\n",
    "    ret = elastic_format(me)\n",
    "    return ret\n",
    "\n",
    "elastic_splade(\"My name is John\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "1. Take in to account the logit values and use the `^` parameter for weights\n",
    "1. Deploy a PYPI package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add path to simple_splade\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../simple_splade\")\n",
    "\n",
    "from elastic_splade import splade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"stevhliu/my_awesome_eli5_mlm_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spalde_model = splade(model_name, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"My name is John\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(his OR my OR this OR i OR a OR the OR s OR our) (am OR m OR id OR name OR named OR title OR by OR start OR time) ( OR am OR was OR is OR s OR a OR are)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spalde_model.splade_it(test_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
